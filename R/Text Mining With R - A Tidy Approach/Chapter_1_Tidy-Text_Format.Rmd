---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.



Chapter 1: A tidy text format

the general idea behind the tidy text format is a one-token-per-row structure.
  the token is usually a single word, but can also be a n-gram, sentance or paragraph.
  

#1.1 Contrasting tidy text with other data structures



```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text
```

To analyse the raw text, we put it into a df:
```{r}
library(dplyr)
text_df <- tibble(line = 1:4, text = text)

text_df
```
tibble is just a modern version of a df, with stricter rules for subsetting but better handling, especially on a larger scale

#1.2 The *unnest_tokens* function
Now to *tokenize* the text we use the function *unnest_tokens*
```{r}
library(tidytext)

text_df %>% unnest_tokens(word, text) # word is the name of the output column
```


#1.3 Tidying the works of Jane Austen

First we organize the text, so that we have a reference for where it originally was in the book (and where in the book specifically), before we tokanize every word individually 
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%                       # group by each novel
  mutate(linenumber = row_number(),       # add line number per book
         chapter = cumsum(                # increment chapter count
           str_detect(text,               # if line matches "chapter x"
             regex("^chapter [\\divxlc]", 
                   ignore_case = TRUE)))) %>%
  ungroup()         

original_books
```

Now with these reference points, we can *tokenize*:
```{r}
library(tidytext)
tidy_books <- original_books %>% 
  unnest_tokens(word, text)

tidy_books

```

Now to remove all the *stop words* ("the", "of", "to" and so on) we use an *anti_join* together with the *stop_words* dataset:
```{r}
data("stop_words") # this is a large dataset of stop words made by tidytext

tidy_books <- tidy_books %>% 
  anti_join(stop_words)

tidy_books
```
We now went from 725,005 words all teh way down to 217,609!

With a clean tokenization, we can start on general analysis:

simple plot of most common word:
```{r}
library(ggplot2)

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


# 1.4 The gutenbergr package 
mainly just how the *gutenberg_download()* from the library *gutenbergr* works, very short


# 1.5 Word Frequencies
a super simple word frequency table set-up:
```{r}
library(gutenbergr)
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))

tidy_bronte <- bronte %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

tidy_bronte %>% 
  count(word, sort = T)
```


now to do it, but to do it from different authers, but being able to tell a difference
```{r}
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)


library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = author, values_from = proportion) %>%
  pivot_longer(`Brontë Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

library(scales)
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```
Words close to the line have similar frequencies in both sets of texts

Now to quantify how similar they are, vo can do a *cor.test* :
```{r}
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)

cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)

```

